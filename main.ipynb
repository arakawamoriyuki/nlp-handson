{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nlp-handson 自然言語処理ハンズオン\n",
    "\n",
    "機械学習フレームワーク keras を用いて自然言語処理(分類)を行います。\n",
    "\n",
    "自然言語分類は、ある文章を、用意したそれぞれの分類にどのくらい当てはまるかを返します。例えばチャットボットのように、ユーザーから得られる無限にある文章のパターンを分類して特定の答えを返すような事ができます。\n",
    "\n",
    "- `main.ipynb` でtensorflow hubのuniversal-sentence-encoderを利用した転移学習で自然言語分類について\n",
    "- `serving.ipynb` でtensorflow servingを利用したAPI化について\n",
    "- `client.ipynb` でそのAPIの利用方法について"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0605 23:15:12.304090 140692289308416 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tf_sentencepiece\n",
    "from keras import backend as K\n",
    "from keras.engine import Layer\n",
    "import keras.layers as layers\n",
    "import keras.optimizers as optimizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用意されたdatasets.csvを読み込み、分類をキーにサンプルデータを用意します。\n",
    "\n",
    "__学習に時間がかかるためdatasets.csvは7分類各5個とかなりスモールデータな例で用意しています。__\n",
    "\n",
    "__パラメータにもよりますが500分類各100個だとCPUでだいたい30分ほどかかります。さらに大きいデータではSagemakerなどのGPUを使った学習を検討しましょう__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、今回は軽く試す程度に止めるため前処理を全くしていません。\n",
    "\n",
    "[自然言語処理における前処理の種類とその威力](https://qiita.com/Hironsan/items/2466fe0f344115aff177)を参考に前処理を検討しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = './datasets.csv'\n",
    "datasets = {}\n",
    "with open(csv_path, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for index, row in enumerate(reader):\n",
    "        collected_row = [sentence for sentence in row if not sentence == '']\n",
    "        if collected_row[0] in datasets:\n",
    "            raise\n",
    "        # 先頭をlabelとする\n",
    "        datasets[collected_row[0]] = collected_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ファッション': ['ファッション',\n",
       "  '帽子買いたい',\n",
       "  'ジーパン欲しい',\n",
       "  'Tシャツ買いたい',\n",
       "  '靴がボロくなった',\n",
       "  'ジャケット買わないと'],\n",
       " 'スーパー': ['スーパー', '夕食の食材買いたい', '野菜買わないと', '卵足りない', '牛乳飲みたい', '日用品がなくなってきた'],\n",
       " 'レストラン': ['レストラン', '腹減った', '飯くいたい', 'ランチどこにしよう', 'ステーキが欲しい', 'お腹ぺこぺこ'],\n",
       " 'コンビニ': ['コンビニ', 'コーヒー飲みたい', 'タバコ買いたい', 'ひと息つきたい', 'お菓子買いたい', '休憩しよう'],\n",
       " 'トイレ': ['トイレ', '漏れそう', '用を足したい', 'お手洗いはどこですか', '化粧室はどこ', '手を洗いたい'],\n",
       " '病院': ['病院', '体調悪い', 'お腹痛い', '頭痛がする', '人間ドック受けたい', '風邪ひいた'],\n",
       " 'ガゾリンスタンド': ['ガゾリンスタンド', '車が止まりそう', 'タイヤがパンクした', 'ガス欠', 'ワイパー交換', 'オイルが古い']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記では人間にわかりやすくデータを整えました。今度はモデルに渡す形式に整えます。\n",
    "\n",
    "xは学習する文章、yはその答えとなるidで、最終的にone hot表現に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, sentences in datasets.items():\n",
    "    labels.append(key)\n",
    "    label_index = labels.index(key)\n",
    "    for sentence in sentences:\n",
    "        x_train.append(sentence)\n",
    "        y_train.append(label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ファッション', '帽子買いたい', 'ジーパン欲しい'], [0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:3], y_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['スーパー', '夕食の食材買いたい', '野菜買わないと'], [1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[6:9], y_train[6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ファッション', 'スーパー', 'レストラン']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(x_train, y_train))\n",
    "shuffle(train_data)\n",
    "x_train = [d[0] for d in train_data]\n",
    "y_train = [d[1] for d in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['靴がボロくなった', '卵足りない', 'ワイパー交換'], [0, 1, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:3], y_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np_utils.to_categorical(np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['靴がボロくなった', '卵足りない', 'ワイパー交換'], dtype='<U11'),\n",
       " array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.]], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:3], y_train[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にモデルの保存先、tensorboardの保存先を指定します。\n",
    "\n",
    "一度学習してモデルを保存している場合、以下のコメントアウトでロードできます。\n",
    "\n",
    "ロードする際、tf.hubを利用するなどkeras外のオブジェクトは保存できない為、再度定義してcustom_objects指定する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model_path = 'models/usex.h5'\n",
    "log_path = 'logs/tboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# model = load_model(model_path, custom_objects={'USEXEmbeddingLayer': USEXEmbeddingLayer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.hubについて\n",
    "\n",
    "tf.hubはtensorflow_hubライブラリを使って[tfhub.dev](https://tfhub.dev/)から学習済みモデルを試す事ができるサービスです。\n",
    "\n",
    "単語ベクトルを取り出す[word2vec](https://tfhub.dev/google/Wiki-words-500-with-normalization/1)や画像分類の[mobilenet](https://tfhub.dev/google/imagenet/mobilenet_v2_035_224/classification/3)などを数行で試す事ができます。\n",
    "\n",
    "転移学習用の学習済みモデルやFine-tuningにも利用できるモデルもあります。\n",
    "\n",
    "今回は[universal-sentence-encoder-xling-many](https://tfhub.dev/google/universal-sentence-encoder-xling-many/1)を利用します。\n",
    "\n",
    "まずは試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0605 23:15:28.501245 140692289308416 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00365919 -0.08720801  0.05245281 ... -0.05964206 -0.07168947\n",
      "  -0.03018713]\n",
      " [-0.01025635 -0.07971002  0.05991658 ... -0.01768638 -0.04235772\n",
      "  -0.00679488]\n",
      " [ 0.05342148 -0.09510113 -0.05290775 ... -0.08235259  0.02876618\n",
      "  -0.07422873]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tf_sentencepiece\n",
    "\n",
    "sentences = [\n",
    "    \"私は犬と一緒にビーチを散歩するのが好きです\",\n",
    "    \"子犬は僕と一緒にビーチを歩くのが好きみたい\",\n",
    "    \"ピザ食べたい\",\n",
    "]\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "  text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "  xling_8_embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-xling-many/1\")\n",
    "  embedded_text = xling_8_embed(text_input)\n",
    "  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "g.finalize()\n",
    "\n",
    "session = tf.Session(graph=g)\n",
    "session.run(init_op)\n",
    "\n",
    "results = session.run(embedded_text, feed_dict={text_input: sentences})\n",
    "\n",
    "session.close()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それぞれの文章(sentence)をuniversal-sentence-encoder-xling-manyに渡し、文章ベクトルを受け取りました。\n",
    "\n",
    "機械学習で分類するには文字もすべて数値型にする必要がありますが、この文章ベクトルが文章の意味を持った数値になります。\n",
    "\n",
    "本当にこの数値が文章の意味を表しているのか少し実験してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8589692\n",
      "0.2273775\n",
      "0.27653313\n"
     ]
    }
   ],
   "source": [
    "def cos_sim(a, b):\n",
    "    return np.inner(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))\n",
    "\n",
    "similarity_matrix0and1 = cos_sim(results[0], results[1])\n",
    "similarity_matrix1and2 = cos_sim(results[1], results[2])\n",
    "similarity_matrix2and0 = cos_sim(results[2], results[0])\n",
    "\n",
    "print(similarity_matrix0and1)\n",
    "print(similarity_matrix1and2)\n",
    "print(similarity_matrix2and0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コサイン類似度を使ってベクトルの近さを測りました。\n",
    "\n",
    "結果は0番目と1番目が文章として近く、2番目は0番目も1番目もかけ離れているようで、意味を持った数値だという事がわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はtf.hubのuniversal-sentence-encoderを使って文章ベクトルを作りました。\n",
    "\n",
    "__本来はmecab+gensim+rnnなどを利用して文章ベクトルを作ります__\n",
    "\n",
    "__分類したい分野の(word2vecなどの教師なし学習に利用する)大量の文章を持っていれば自分で学習した方が効率がいいかもしれません。例えば自社で料理に関する文章を大量に持っている場合に、料理に関する自然言語分類をしたい時などです。__\n",
    "\n",
    "__スモールデータの場合はtf.hubなどを使った転移学習は有効です。__\n",
    "\n",
    "nnlm、word2vec、elmo、universal-sentence-encoderを試しましたが、私が持っている問題に関しては一番universal-sentence-encoderが優秀でした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.hubを利用して転移学習するためのモデルの層を定義します。\n",
    "\n",
    "ややこしく書いていますが、単に学習済みモデルを利用した文章ベクトルを取り出した結果を利用しています。(以下のようなLambdaで定義しても同様です)\n",
    "\n",
    "```python\n",
    "def create_embedding_lambda():\n",
    "    embed = hub.Module(\n",
    "        'https://tfhub.dev/google/universal-sentence-encoder-xling-many/1',\n",
    "        trainable=False,\n",
    "        name='USEXEmbeddingLayer_module',\n",
    "    )\n",
    "    def embedding(x):\n",
    "        return embed(\n",
    "            K.squeeze(K.cast(x, K.tf.string), axis=1),\n",
    "            signature=\"default\",\n",
    "            as_dict=True,\n",
    "        )[\"default\"]\n",
    "    return embedding\n",
    "\n",
    "outputs = layers.Lambda(create_embedding_lambda(), output_shape=(512,))(input_text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、未検証ですが、tensorflow v2からは[tf.hub.KerasLayer](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer)に対応したモデルであればカスタムレイヤ作る必要がなくなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define EmbeddingLayer\n",
    "class USEXEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.name = 'USEXEmbeddingLayer'\n",
    "        self.trainable = kwargs['trainable'] if 'trainable' in kwargs else False\n",
    "        super(USEXEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.usex = hub.Module(\n",
    "            'https://tfhub.dev/google/universal-sentence-encoder-xling-many/1',\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name),\n",
    "        )\n",
    "        super(USEXEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        result = self.usex(\n",
    "            K.squeeze(K.cast(x, K.tf.string), axis=1),\n",
    "            as_dict=True,\n",
    "            signature='default',\n",
    "        )['default']\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したレイヤを用いてモデルを定義します。\n",
    "\n",
    "独自レイヤを使っている以外は単純な全層結合だけです。\n",
    "\n",
    "合間にランダムにノードを不活性化させるDropoutやバッチごとに正規化するBatchNormalizationを用いて過学習を抑制します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0605 23:15:50.582492 140692289308416 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "outputs = USEXEmbeddingLayer()(inputs)\n",
    "outputs = layers.Dense(512, activation='relu')(outputs)\n",
    "outputs = layers.BatchNormalization()(outputs)\n",
    "outputs = layers.Dropout(0.5)(outputs)\n",
    "outputs = layers.Dense(512, activation='relu')(outputs)\n",
    "outputs = layers.BatchNormalization()(outputs)\n",
    "outputs = layers.Dropout(0.5)(outputs)\n",
    "outputs = layers.Dense(len(labels), activation='softmax')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "usex_embedding_layer_1 (USEX (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 532,999\n",
      "Trainable params: 530,951\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[inputs], outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=optimizers.rmsprop(\n",
    "        lr=0.001,\n",
    "        rho=0.9,\n",
    "        epsilon=None,\n",
    "        decay=0.0,\n",
    "    ),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['acc'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際にトレーニングしてみましょう。\n",
    "\n",
    "過学習し始めた際に学習を止めるEarlyStoppingや学習率を学習中に変更してくれるReduceLROnPlateauなどをcallbackに使ってます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37 samples, validate on 5 samples\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 4s 113ms/step - loss: 3.2915 - acc: 0.0270 - val_loss: 1.1783 - val_acc: 0.8000\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8989 - acc: 0.7027 - val_loss: 1.1652 - val_acc: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff47a1676d8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=1024,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor='val_acc',\n",
    "            patience=1,\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=model_path,\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.3,\n",
    "            patience=1,\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=log_path,\n",
    "            write_graph=True,\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を終えたのでためしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "sentence = 'ディナーどこにしよう'\n",
    "\n",
    "results = model.predict(np.array([sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4555707e-02, 1.2063762e-02, 9.2107445e-01, 1.8246198e-02,\n",
       "        2.6356930e-02, 6.8286220e-03, 8.7419694e-04]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.11%:レストラン\n",
      "2.64%:トイレ\n",
      "1.82%:コンビニ\n"
     ]
    }
   ],
   "source": [
    "result = results[0]\n",
    "indexes = list(range(len(labels)))\n",
    "predictions = dict(zip(indexes, result))\n",
    "predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "predictions = predictions[0:3]\n",
    "for prediction in predictions:\n",
    "    label = labels[prediction[0]]\n",
    "    score = '{:.2%}'.format(prediction[1])\n",
    "    print('{}:{}'.format(score, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predicter(model, labels):\n",
    "    indexes = list(range(len(labels)))\n",
    "\n",
    "    def predicter(sentences):\n",
    "        results = model.predict(np.array(sentences))\n",
    "        for sentence_index, result in enumerate(results):\n",
    "            sentence = sentences[sentence_index]\n",
    "            print('====================')\n",
    "            print('q: {}'.format(sentence))\n",
    "            predictions = dict(zip(indexes, result))\n",
    "            predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "            predictions = predictions[0:5]\n",
    "            for prediction in predictions:\n",
    "                index = prediction[0]\n",
    "                label = labels[index]\n",
    "                score = prediction[1]\n",
    "                print('\\n----------\\nscore:{}\\n{}'.format('{:.2%}'.format(score), label))\n",
    "\n",
    "    return predicter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicter = generate_predicter(model, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "q: 車が壊れた\n",
      "\n",
      "----------\n",
      "score:81.57%\n",
      "ガゾリンスタンド\n",
      "\n",
      "----------\n",
      "score:11.88%\n",
      "ファッション\n",
      "\n",
      "----------\n",
      "score:3.12%\n",
      "病院\n",
      "\n",
      "----------\n",
      "score:1.25%\n",
      "コンビニ\n",
      "\n",
      "----------\n",
      "score:1.03%\n",
      "トイレ\n"
     ]
    }
   ],
   "source": [
    "predicter(['車が壊れた'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API化する為にtensorflow servingに対応したprotocol buffers形式で保存し、serving.ipynbにうつりましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/simple_save.py:85: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 23:16:29.572692 140692289308416 tf_logging.py:125] From /usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/simple_save.py:85: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0605 23:16:29.578189 140692289308416 tf_logging.py:115] Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0605 23:16:29.581063 140692289308416 tf_logging.py:115] No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: models/serving/1/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0605 23:16:46.790414 140692289308416 tf_logging.py:115] SavedModel written to: models/serving/1/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "# save pb\n",
    "serving_model_path = 'models/serving/1'\n",
    "tf.saved_model.simple_save(\n",
    "    K.get_session(),\n",
    "    serving_model_path,\n",
    "    inputs={'inputs': model.input},\n",
    "    outputs={t.name: t for t in model.outputs},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
