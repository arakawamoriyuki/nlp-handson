{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0605 19:21:05.651664 140396928567040 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tf_sentencepiece\n",
    "from keras import backend as K\n",
    "from keras.engine import Layer\n",
    "import keras.layers as layers\n",
    "import keras.optimizers as optimizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "datasets = {\n",
    "    'ファッション': ['帽子買いたい', 'ジーパン欲しい', 'Tシャツ買いたい', '靴がボロくなった', 'ジャケット買わないと'],\n",
    "    'スーパー': ['夕食の食材買いたい', '野菜買わないと', '卵足りない', '牛乳飲みたい', '日用品がなくなってきた'],\n",
    "    'レストラン': ['腹減った', '飯くいたい', 'ランチどこにしよう', 'ステーキが欲しい', 'お腹ぺこぺこ'],\n",
    "    'コンビニ': ['コーヒー飲みたい', 'タバコ買いたい', 'ひと息つきたい', 'お菓子買いたい', '休憩しよう'],\n",
    "    'トイレ': ['漏れそう', '用を足したい', 'お手洗いはどこですか', '化粧室はどこ', '手を洗いたい'],\n",
    "    '病院': ['体調悪い', 'お腹痛い', '頭痛がする', '人間ドック受けたい', '風邪ひいた'],\n",
    "    'ガゾリンスタンド': ['車が止まりそう', 'タイヤがパンクした', 'ガス欠', 'ワイパー交換', 'オイルが古い'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, sentences in datasets.items():\n",
    "    labels.append(key)\n",
    "    label_index = labels.index(key)\n",
    "    for sentence in sentences:\n",
    "        x_train.append(sentence)\n",
    "        y_train.append(label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['帽子買いたい',\n",
       "  'ジーパン欲しい',\n",
       "  'Tシャツ買いたい',\n",
       "  '靴がボロくなった',\n",
       "  'ジャケット買わないと',\n",
       "  '夕食の食材買いたい',\n",
       "  '野菜買わないと',\n",
       "  '卵足りない',\n",
       "  '牛乳飲みたい',\n",
       "  '日用品がなくなってきた',\n",
       "  '腹減った',\n",
       "  '飯くいたい',\n",
       "  'ランチどこにしよう',\n",
       "  'ステーキが欲しい',\n",
       "  'お腹ぺこぺこ',\n",
       "  'コーヒー飲みたい',\n",
       "  'タバコ買いたい',\n",
       "  'ひと息つきたい',\n",
       "  'お菓子買いたい',\n",
       "  '休憩しよう',\n",
       "  '漏れそう',\n",
       "  '用を足したい',\n",
       "  'お手洗いはどこですか',\n",
       "  '化粧室はどこ',\n",
       "  '手を洗いたい',\n",
       "  '体調悪い',\n",
       "  'お腹痛い',\n",
       "  '頭痛がする',\n",
       "  '人間ドック受けたい',\n",
       "  '風邪ひいた',\n",
       "  '車が止まりそう',\n",
       "  'タイヤがパンクした',\n",
       "  'ガス欠',\n",
       "  'ワイパー交換',\n",
       "  'オイルが古い'],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6],\n",
       " ['ファッション', 'スーパー', 'レストラン', 'コンビニ', 'トイレ', '病院', 'ガゾリンスタンド'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(x_train, y_train))\n",
    "shuffle(train_data)\n",
    "x_train = [d[0] for d in train_data]\n",
    "y_train = [d[1] for d in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Tシャツ買いたい',\n",
       "  '人間ドック受けたい',\n",
       "  '夕食の食材買いたい',\n",
       "  '靴がボロくなった',\n",
       "  'お腹ぺこぺこ',\n",
       "  '野菜買わないと',\n",
       "  '卵足りない',\n",
       "  '帽子買いたい',\n",
       "  'オイルが古い',\n",
       "  '休憩しよう',\n",
       "  'ジャケット買わないと',\n",
       "  'タバコ買いたい',\n",
       "  '手を洗いたい',\n",
       "  '牛乳飲みたい',\n",
       "  '風邪ひいた',\n",
       "  'ガス欠',\n",
       "  'ステーキが欲しい',\n",
       "  '漏れそう',\n",
       "  '頭痛がする',\n",
       "  'タイヤがパンクした',\n",
       "  'ジーパン欲しい',\n",
       "  'お菓子買いたい',\n",
       "  '車が止まりそう',\n",
       "  '化粧室はどこ',\n",
       "  '体調悪い',\n",
       "  'コーヒー飲みたい',\n",
       "  '日用品がなくなってきた',\n",
       "  'ひと息つきたい',\n",
       "  'ランチどこにしよう',\n",
       "  'お腹痛い',\n",
       "  '用を足したい',\n",
       "  '腹減った',\n",
       "  'ワイパー交換',\n",
       "  '飯くいたい',\n",
       "  'お手洗いはどこですか'],\n",
       " [0,\n",
       "  5,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  6,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  6,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  0,\n",
       "  3,\n",
       "  6,\n",
       "  4,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  4,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  4],\n",
       " ['ファッション', 'スーパー', 'レストラン', 'コンビニ', 'トイレ', '病院', 'ガゾリンスタンド'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np_utils.to_categorical(np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Tシャツ買いたい', '人間ドック受けたい', '夕食の食材買いたい', '靴がボロくなった', 'お腹ぺこぺこ',\n",
       "        '野菜買わないと', '卵足りない', '帽子買いたい', 'オイルが古い', '休憩しよう', 'ジャケット買わないと',\n",
       "        'タバコ買いたい', '手を洗いたい', '牛乳飲みたい', '風邪ひいた', 'ガス欠', 'ステーキが欲しい', '漏れそう',\n",
       "        '頭痛がする', 'タイヤがパンクした', 'ジーパン欲しい', 'お菓子買いたい', '車が止まりそう', '化粧室はどこ',\n",
       "        '体調悪い', 'コーヒー飲みたい', '日用品がなくなってきた', 'ひと息つきたい', 'ランチどこにしよう', 'お腹痛い',\n",
       "        '用を足したい', '腹減った', 'ワイパー交換', '飯くいたい', 'お手洗いはどこですか'], dtype='<U11'),\n",
       " array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.]], dtype=float32),\n",
       " ['ファッション', 'スーパー', 'レストラン', 'コンビニ', 'トイレ', '病院', 'ガゾリンスタンド'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model_path = 'models/usex.h5'\n",
    "log_path = 'logs/tboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# model = load_model(model_path, custom_objects={'USEXEmbeddingLayer': USEXEmbeddingLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define EmbeddingLayer\n",
    "class USEXEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.name = 'USEXEmbeddingLayer'\n",
    "        self.trainable = kwargs['trainable'] if 'trainable' in kwargs else False\n",
    "        super(USEXEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.usex = hub.Module(\n",
    "            'https://tfhub.dev/google/universal-sentence-encoder-xling-many/1',\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name),\n",
    "        )\n",
    "        super(USEXEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        result = self.usex(\n",
    "            K.squeeze(K.cast(x, K.tf.string), axis=1),\n",
    "            as_dict=True,\n",
    "            signature='default',\n",
    "        )['default']\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0605 19:21:24.054427 140396928567040 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "outputs = USEXEmbeddingLayer()(inputs)\n",
    "outputs = layers.Dense(512, activation='relu')(outputs)\n",
    "outputs = layers.BatchNormalization()(outputs)\n",
    "outputs = layers.Dropout(0.5)(outputs)\n",
    "outputs = layers.Dense(512, activation='relu')(outputs)\n",
    "outputs = layers.BatchNormalization()(outputs)\n",
    "outputs = layers.Dropout(0.5)(outputs)\n",
    "outputs = layers.Dense(len(labels), activation='softmax')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "usex_embedding_layer_1 (USEX (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 532,999\n",
      "Trainable params: 530,951\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[inputs], outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=optimizers.rmsprop(\n",
    "        lr=0.001,\n",
    "        rho=0.9,\n",
    "        epsilon=None,\n",
    "        decay=0.0,\n",
    "    ),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['acc'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31 samples, validate on 4 samples\n",
      "Epoch 1/100\n",
      "31/31 [==============================] - 3s 108ms/step - loss: 2.9753 - acc: 0.1613 - val_loss: 1.3363 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.7749 - acc: 0.7097 - val_loss: 1.2290 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb029564630>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=1024,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor='val_acc',\n",
    "            patience=1,\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=model_path,\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.3,\n",
    "            patience=1,\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=log_path,\n",
    "            write_graph=True,\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "sentence = 'ディナーどこにしよう'\n",
    "\n",
    "results = model.predict(np.array([sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01208884, 0.22686645, 0.6707035 , 0.02477913, 0.04699144,\n",
       "        0.00087108, 0.01769963]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.07%:レストラン\n",
      "22.69%:スーパー\n",
      "4.70%:トイレ\n"
     ]
    }
   ],
   "source": [
    "result = results[0]\n",
    "indexes = list(range(len(labels)))\n",
    "predictions = dict(zip(indexes, result))\n",
    "predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "predictions = predictions[0:3]\n",
    "for prediction in predictions:\n",
    "    label = labels[prediction[0]]\n",
    "    score = '{:.2%}'.format(prediction[1])\n",
    "    print('{}:{}'.format(score, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predicter(model, labels):\n",
    "    indexes = list(range(len(labels)))\n",
    "\n",
    "    def predicter(sentences):\n",
    "        results = model.predict(np.array(sentences))\n",
    "        for sentence_index, result in enumerate(results):\n",
    "            sentence = sentences[sentence_index]\n",
    "            print('====================')\n",
    "            print('q: {}'.format(sentence))\n",
    "            predictions = dict(zip(indexes, result))\n",
    "            predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "            predictions = predictions[0:5]\n",
    "            for prediction in predictions:\n",
    "                index = prediction[0]\n",
    "                label = labels[index]\n",
    "                score = prediction[1]\n",
    "                print('\\n----------\\nscore:{}\\n{}'.format('{:.2%}'.format(score), label))\n",
    "\n",
    "    return predicter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicter = generate_predicter(model, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "q: 車が壊れた\n",
      "\n",
      "----------\n",
      "score:99.96%\n",
      "ガゾリンスタンド\n",
      "\n",
      "----------\n",
      "score:0.02%\n",
      "スーパー\n",
      "\n",
      "----------\n",
      "score:0.01%\n",
      "トイレ\n",
      "\n",
      "----------\n",
      "score:0.01%\n",
      "ファッション\n",
      "\n",
      "----------\n",
      "score:0.00%\n",
      "コンビニ\n"
     ]
    }
   ],
   "source": [
    "predicter(['車が壊れた'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/simple_save.py:85: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 19:22:34.177600 140396928567040 tf_logging.py:125] From /usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/simple_save.py:85: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0605 19:22:34.179158 140396928567040 tf_logging.py:115] Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0605 19:22:34.180423 140396928567040 tf_logging.py:115] No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: models/serving/1/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0605 19:22:48.280967 140396928567040 tf_logging.py:115] SavedModel written to: models/serving/1/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "# save pb\n",
    "serving_model_path = 'models/serving/1'\n",
    "tf.saved_model.simple_save(\n",
    "    K.get_session(),\n",
    "    serving_model_path,\n",
    "    inputs={'inputs': model.input},\n",
    "    outputs={t.name: t for t in model.outputs},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
